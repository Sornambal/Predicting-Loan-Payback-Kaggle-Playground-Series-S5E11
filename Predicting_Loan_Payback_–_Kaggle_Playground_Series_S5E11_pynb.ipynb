{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sornambal/Predicting-Loan-Payback-Kaggle-Playground-Series-S5E11/blob/main/Predicting_Loan_Payback_%E2%80%93_Kaggle_Playground_Series_S5E11_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "gc – Python’s garbage collector, used to free memory manually (gc.collect()).\n",
        "\n",
        "warnings.filterwarnings(\"ignore\") – hides non-critical warnings for cleaner logs.\n",
        "\n",
        "numpy, pandas – standard libraries for numerical operations and tabular data.\n",
        "\n",
        "StratifiedKFold – splits the data into folds while preserving the class ratio (important for classification).\n",
        "\n",
        "SimpleImputer – fills missing values.\n",
        "\n",
        "OrdinalEncoder – converts categorical features to integer codes.\n",
        "\n",
        "roc_auc_score – metric used in this competition (AUC).\n",
        "\n",
        "compute_class_weight – computes weights for imbalanced classes.\n",
        "\n",
        "lightgbm – main model: Gradient Boosted Decision Trees (LGBMClassifier)."
      ],
      "metadata": {
        "id": "H75SXDVTa6xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gc\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Jug4lAtgZn54"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "Defines file paths for train.csv, test.csv, and the output submission.csv.\n",
        "\n",
        "target_col is the label to predict (loan_paid_back).\n",
        "\n",
        "id_col is the row identifier (id) required in the submission."
      ],
      "metadata": {
        "id": "Q7wf58AIbLF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TRAIN_PATH = \"/content/train.csv\"\n",
        "TEST_PATH  = \"/content/test.csv\"\n",
        "SUBMISSION_PATH = \"submission_lgbm_multiseed.csv\"\n",
        "\n",
        "target_col = \"loan_paid_back\"\n",
        "id_col = \"id\""
      ],
      "metadata": {
        "id": "S8Vd4ZhhZtOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading data ...\")\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test  = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Test shape :\", test.shape)\n",
        "\n",
        "# Drop NaN targets if any\n",
        "if train[target_col].isnull().any():\n",
        "    print(f\"Dropping {train[target_col].isnull().sum()} rows with NaN target\")\n",
        "    train = train.dropna(subset=[target_col]).reset_index(drop=True)\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "train_ids = train[id_col].values\n",
        "test_ids  = test[id_col].values\n"
      ],
      "metadata": {
        "id": "wNpqDzknbmFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "Reads train.csv and test.csv into pandas DataFrames.\n",
        "\n",
        "Prints their shapes for a quick sanity check.\n",
        "\n",
        "If any rows in the target column are NaN, they are dropped (can happen in some Kaggle datasets).\n",
        "\n",
        "y – numpy array of target labels.\n",
        "\n",
        "train_ids, test_ids – store id values for later use in submission."
      ],
      "metadata": {
        "id": "1oYwqK--bdxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= CLASS WEIGHTS =========\n",
        "classes = np.unique(y)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
        "class_weight_dict = {cls: w for cls, w in zip(classes, cw)}\n",
        "print(\"Class weights:\", class_weight_dict)\n"
      ],
      "metadata": {
        "id": "s_CWmydqbwXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**:\n",
        "\n",
        "Computes class weights so the model gives equal importance to both classes (0 and 1), even if they are imbalanced.\n",
        "\n",
        "These weights are passed into LightGBM as class_weight to improve performance on the minority class."
      ],
      "metadata": {
        "id": "3MmNDf5Rb0go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= DETECT FEATURES =========\n",
        "cat_cols = [c for c in train.columns\n",
        "            if train[c].dtype == \"object\"\n",
        "            and c not in [target_col, id_col]]\n",
        "\n",
        "for c in train.select_dtypes(include=[\"int64\", \"int32\"]).columns:\n",
        "    if c in [id_col, target_col]:\n",
        "        continue\n",
        "    if train[c].nunique() < 30:\n",
        "        cat_cols.append(c)\n",
        "\n",
        "cat_cols = sorted(list(set(cat_cols)))\n",
        "num_cols = [c for c in train.columns if c not in cat_cols + [target_col, id_col]]\n",
        "\n",
        "print(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")\n",
        "print(f\"Numeric columns      ({len(num_cols)}): {num_cols[:10]}{' ...' if len(num_cols) > 10 else ''}\")\n"
      ],
      "metadata": {
        "id": "OHUy4Pflb-Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Explanation:**\n",
        "\n",
        "Detects categorical columns in two ways:\n",
        "\n",
        "dtype == object → text-like columns.\n",
        "\n",
        "Integer columns with small number of unique values (< 30) → treated as categorical (e.g., codes, flags).\n",
        "\n",
        "cat_cols – list of categorical feature names.\n",
        "\n",
        "num_cols – all remaining features (numeric).\n",
        "\n",
        "This dynamic detection makes the script more general."
      ],
      "metadata": {
        "id": "gNWsRyaDcG6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= PREPROCESSING =========\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")\n",
        "ordinal_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "# concatenate train+test for consistent transforms\n",
        "full = pd.concat([train.drop(columns=[target_col]), test], axis=0, ignore_index=True)\n",
        "\n",
        "full_num = pd.DataFrame(num_imputer.fit_transform(full[num_cols]), columns=num_cols)\n",
        "full_cat = pd.DataFrame(cat_imputer.fit_transform(full[cat_cols]), columns=cat_cols)\n",
        "\n",
        "if cat_cols:\n",
        "    full_cat_enc = pd.DataFrame(\n",
        "        ordinal_enc.fit_transform(full_cat),\n",
        "        columns=cat_cols\n",
        "    ).astype(np.int32)\n",
        "else:\n",
        "    full_cat_enc = pd.DataFrame(index=full.index)\n",
        "\n",
        "full_processed = pd.concat(\n",
        "    [full_num.reset_index(drop=True), full_cat_enc.reset_index(drop=True)],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "X_all  = full_processed.iloc[:len(train), :].reset_index(drop=True)\n",
        "X_test = full_processed.iloc[len(train):, :].reset_index(drop=True)\n",
        "\n",
        "print(\"Processed train shape:\", X_all.shape)\n",
        "print(\"Processed test shape :\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "klKag0efcPeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "SimpleImputer:\n",
        "\n",
        "For numeric columns: median imputation.\n",
        "\n",
        "For categorical columns: fill missing values with \"__MISSING__\".\n",
        "\n",
        "OrdinalEncoder:\n",
        "\n",
        "Converts categories to integer codes,\n",
        "\n",
        "handle_unknown=\"use_encoded_value\", unknown_value=-1 prevents crashes if test has unseen categories.\n",
        "\n",
        "train and test are concatenated into full so that imputation and encoding are consistent across both.\n",
        "\n",
        "After preprocessing:\n",
        "\n",
        "X_all – model features for training (rows from original train).\n",
        "\n",
        "X_test – model features for test set."
      ],
      "metadata": {
        "id": "O2MzUvfacTgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= LIGHTGBM PARAMS TEMPLATE =========\n",
        "def get_lgb_params(seed):\n",
        "    return {\n",
        "        \"objective\": \"binary\",\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"metric\": \"auc\",\n",
        "        \"learning_rate\": 0.015,\n",
        "        \"num_leaves\": 63,\n",
        "        \"max_depth\": -1,\n",
        "        \"min_child_samples\": 60,\n",
        "        \"min_gain_to_split\": 0.01,\n",
        "        \"feature_fraction\": 0.7,   # colsample_bytree\n",
        "        \"bagging_fraction\": 0.8,   # subsample\n",
        "        \"bagging_freq\": 1,\n",
        "        \"lambda_l1\": 0.3,\n",
        "        \"lambda_l2\": 0.6,\n",
        "        \"max_bin\": 255,\n",
        "        \"n_estimators\": 7000,\n",
        "        \"n_jobs\": -1,\n",
        "        \"verbose\": -1,\n",
        "        \"force_col_wise\": True,\n",
        "        \"class_weight\": class_weight_dict,\n",
        "        \"random_state\": seed,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "38tMxhCNcXw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "Defines a function that returns a LightGBM parameter dictionary for a given seed.\n",
        "\n",
        "Important choices:\n",
        "\n",
        "learning_rate = 0.015 – relatively low for better generalization.\n",
        "\n",
        "num_leaves = 63 – tree complexity.\n",
        "\n",
        "feature_fraction, bagging_fraction – column and row sampling for regularization.\n",
        "\n",
        "lambda_l1, lambda_l2 – L1 & L2 regularization to reduce overfitting.\n",
        "\n",
        "n_estimators = 7000 – large upper bound on trees; actual used trees are controlled by early stopping.\n",
        "\n",
        "class_weight = class_weight_dict – handles class imbalance.\n",
        "\n",
        "random_state = seed – makes the model reproducible per seed."
      ],
      "metadata": {
        "id": "nVo4z9wNccnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= MULTI-SEED CV TRAINING =========\n",
        "SEEDS = [42, 2024, 7]   # you can tweak seeds here\n",
        "NFOLD = 5\n",
        "\n",
        "oof_blend = np.zeros(len(train))\n",
        "test_blend = np.zeros(len(test))\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n================== SEED {seed} ==================\")\n",
        "    params = get_lgb_params(seed)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=seed)\n",
        "\n",
        "    oof_seed = np.zeros(len(train))\n",
        "    test_seed = np.zeros(len(test))\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_all, y), 1):\n",
        "        print(f\"\\n--- Seed {seed} | Fold {fold}/{NFOLD} ---\")\n",
        "\n",
        "        X_tr, X_val = X_all.iloc[tr_idx], X_all.iloc[val_idx]\n",
        "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "        model = lgb.LGBMClassifier(**params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=\"auc\",\n",
        "            callbacks=[lgb.early_stopping(200, verbose=True)]\n",
        "        )\n",
        "\n",
        "        val_pred = model.predict_proba(X_val)[:, 1]\n",
        "        test_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        oof_seed[val_idx] = val_pred\n",
        "        test_seed += test_pred / NFOLD\n",
        "\n",
        "        fold_auc = roc_auc_score(y_val, val_pred)\n",
        "        print(f\"Seed {seed} | Fold {fold} AUC: {fold_auc:.6f}\")\n",
        "\n",
        "        del model, X_tr, X_val, y_tr, y_val\n",
        "        gc.collect()\n",
        "\n",
        "    seed_auc = roc_auc_score(y, oof_seed)\n",
        "    print(f\"\\nSeed {seed} OOF AUC: {seed_auc:.6f}\")\n",
        "\n",
        "    oof_blend += oof_seed / len(SEEDS)\n",
        "    test_blend += test_seed / len(SEEDS)\n"
      ],
      "metadata": {
        "id": "xZjvHBnzciNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "SEEDS = [42, 2024, 7]: the same model is trained 3 times with different random seeds.\n",
        "\n",
        "This is a standard Kaggle trick called multi-seed ensembling.\n",
        "\n",
        "NFOLD = 5: 5-fold StratifiedKFold cross-validation for each seed.\n",
        "\n",
        "For each seed:\n",
        "\n",
        "Create new LightGBM parameters with that seed.\n",
        "\n",
        "Do 5-fold CV:\n",
        "\n",
        "Split into training and validation sets.\n",
        "\n",
        "Train LightGBM with early stopping (early_stopping(200)).\n",
        "\n",
        "Generate:\n",
        "\n",
        "val_pred → out-of-fold predictions.\n",
        "\n",
        "test_pred → test predictions for that fold.\n",
        "\n",
        "For each seed:\n",
        "\n",
        "oof_seed collects the OOF predictions.\n",
        "\n",
        "test_seed is the average of test predictions over folds.\n",
        "\n",
        "After each seed:\n",
        "\n",
        "Compute seed_auc (OOF AUC for that seed).\n",
        "\n",
        "Add oof_seed and test_seed into global blended arrays, averaging over number of seeds.\n",
        "\n",
        "This reduces variance and usually improves leaderboard performance slightly."
      ],
      "metadata": {
        "id": "BCC6-Mbucjdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_oof_auc = roc_auc_score(y, oof_blend)\n",
        "print(f\"\\n================== FINAL BLENDED OOF AUC: {final_oof_auc:.6f} ==================\")\n",
        "\n",
        "# ========= SUBMISSION =========\n",
        "final_pred = np.clip(test_blend, 0.0, 1.0)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    id_col: test_ids,\n",
        "    target_col: final_pred\n",
        "})\n",
        "\n",
        "submission.to_csv(SUBMISSION_PATH, index=False)\n",
        "print(f\"\\nSaved submission to {SUBMISSION_PATH}\")\n",
        "print(\"Finished at\", datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\"))\n"
      ],
      "metadata": {
        "id": "EM4cGPOYcwSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "final_oof_auc: AUC on the entire training set using the blended OOF predictions from all seeds.\n",
        "\n",
        "This is your best internal estimate of how good the model is.\n",
        "\n",
        "np.clip(test_blend, 0.0, 1.0): ensures no predicted probability goes outside [0, 1] (safety).\n",
        "\n",
        "Creates the submission DataFrame with:\n",
        "\n",
        "id column\n",
        "\n",
        "loan_paid_back (predicted probabilities)\n",
        "\n",
        "Saves submission_lgbm_multiseed.csv in the required Kaggle format.\n",
        "\n",
        "Prints the finish time."
      ],
      "metadata": {
        "id": "KJTGb7N8czk2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSZwIgSBZvOAyUL9bySjqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}